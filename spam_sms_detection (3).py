# -*- coding: utf-8 -*-
"""spam_sms_detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1g8GfY7iedgrxD6nYHjdsxamSpRudiVlG
"""

#importing all the important libraries for the developement of the model as well as the machine learning models through the scikit learn library.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC

#importing the dataset in colab through the pandas library, with the encoding as encoding is in latin.
spam = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/spam.csv', encoding = 'latin-1')

#dataset before removing unnecessary columns.
spam

#dropping all the unnecessary columns from the dataset using the drop function.
spam = spam.drop(labels = {'Unnamed: 2',	'Unnamed: 3',	'Unnamed: 4'}, axis = 1)

#datsaset after removing unnecessary columns.
spam

#displaying first five rows of the dataset using the head function or method.
spam.head(5)

#info of the dataset.
spam.info()

#countplot of target attribute giving the count of spam sms and ham sms.
sns.countplot(x = 'v1', data = spam)

#converting spam and ham variables into binary format.
spam['v1'] = spam['v1'].replace({'ham' : 0, 'spam' : 1})

#predictor column
spam['v2']

#using the tf-idf technique to count frequency of a particular word and to check whether that word is important or not.
sample_text = ['This is the first copy.','This is the second copy.','And this is the third one.','Is this the first copy?']
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(sample_text)

#X has been converted to sparse matrix using the tf-idf vectorizer.
X

#displaying the values of X after converting the sparse matrix to normal array.
print(X.toarray())

#displaying the features names of the demo sentence given above.
print(vectorizer.get_feature_names_out())

#now fitting the predictor attribute in the vectorizer so as to get the vectors of sms.
spam_message = vectorizer.fit_transform(spam['v2'])

#shape of sms after creating the vectors.
spam_message.shape

##displaying the features names of the sms.
print(vectorizer.get_feature_names_out())

#displaying the array
print(spam_message.toarray())

#dropping the v2 attribute that contains the sms and adding the vectorized attribute to the dataset.
spam.drop(['v2'], axis = 1, inplace = True)

#converting and adding the vectorized sms into a dataframe.
spam_ms = pd.DataFrame(spam_message.toarray())

#joining the dataframes.
spam = pd.concat([spam,spam_ms], axis = 1)

#new dataframe after combining the above two.
spam

#dropping the target attribute from the predictor variable.
X = spam.drop(labels = {'v1'}, axis = 1)

#assigning v1 attributee to the target attribute.
y = spam['v1']

y

#splitting the dataset into training and testing set.
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2)

#displaying the shape of the both training and testing dataset.
print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)

#imported LogisticRegression() model directly through the scikit learn library specified in cell number 1.
model1 = LogisticRegression()
model1.fit(X_train,y_train)
print("Training_Score:",model1.score(X_train, y_train)*100)
print("Testing_score :",model1.score(X_test, y_test)*100)

#imported RandomForestClassifier() model directly through the scikit learn library specified in cell number 1.
model2 = RandomForestClassifier()
model2.fit(X_train,y_train)
print("Training_Score:",model2.score(X_train, y_train)*100)
print("Testing_score :",model2.score(X_test, y_test)*100)

#imported GaussianNB() model directly through the scikit learn library specified in cell number 1.
model3 = GaussianNB()
model3.fit(X_train,y_train)
print("Training_Score:",model3.score(X_train, y_train)*100)
print("Testing_score :",model3.score(X_test, y_test)*100)

#imported Support vector Machine aka SVC() model directly through the scikit learn library specified in cell number 1.
model4 = SVC()
model4.fit(X_train, y_train)
print("Training_Score:",model4.score(X_train, y_train)*100)
print("Testing_score :",model4.score(X_test, y_test)*100)

#imported Ann through keras API and tensorflow
#ANN is just to check how the it performs on the dataset
classifier_model = tf.keras.models.Sequential()
classifier_model.add(tf.keras.layers.Dense(units = 100, activation = 'relu', input_shape = (8672,)))
classifier_model.add(tf.keras.layers.Dropout(0.3))
classifier_model.add(tf.keras.layers.Dense(units = 50, activation = 'relu'))
classifier_model.add(tf.keras.layers.Dropout(0.3))
classifier_model.add(tf.keras.layers.Dense(units = 50, activation = 'relu'))
classifier_model.add(tf.keras.layers.Dense(units = 1, activation = 'sigmoid'))

#compiled Ann model
classifier_model.compile(optimizer = 'Adam', loss = 'binary_crossentropy', metrics = 'accuracy')

#calculating the epochs on training dataset
epochs_hist = classifier_model.fit(X_train,y_train,epochs = 5,batch_size = 125)

#calculating the epochs on testing dataset
epochs_hist = classifier_model.fit(X_test,y_test,epochs = 5,batch_size = 125)

#finding out the keys of epochs
epochs_hist.history.keys()

#plotting the graph between the training loss and accuracy vs number epochs.
eh = epochs_hist.history['loss']
eh2 = epochs_hist.history['accuracy']
plt.plot(eh)
plt.plot(eh2)
plt.title('Training loss and Accuracy vs epochs graph')
plt.xlabel('epochs')
plt.ylabel('Training loss and Accuracy')
plt.legend({'accuracy', 'loss'})

#evaluating the model on the testing dataset.
evaluation = classifier_model.evaluate(X_test,y_test)
print('test accuracy:{}'.format(evaluation[1]))

#model4 is SVM()
y_predict = model4.predict(X_test)

print(y_predict)

#filtering those values greater than 0.5 in testing dataset.
y_predict = (y_predict > 0.5)
y_predict

#model4 is SVM()
y_train_predict = model4.predict(X_train)

print(y_train_predict)

#filtering those values greater than 0.5 in testing dataset.
y_train_predict = (y_train_predict > 0.5)
y_train_predict

#plotting the confusion matrix of training dataset
from sklearn.metrics import confusion_matrix,classification_report
cm = confusion_matrix(y_train, y_train_predict)
sns.heatmap(cm, annot = True)

#plotting the confusion matrix of testing dataset
cm2 = confusion_matrix(y_test, y_predict)
sns.heatmap(cm2, annot = True)

#printing the classification report of training and testing dataset
print("Training Report:\n",classification_report(y_train,y_train_predict))
print("Testing Report:\n",classification_report(y_test, y_predict))